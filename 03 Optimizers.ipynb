{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithms\n",
    "\n",
    "In the previous experiment, we have explored some variations of Gradient Descent, particularly, __mini-Batch Gradient Descent (mBGD)__, and __Stochastic Gradient Descent (SGD)__. Mini-batch Gradient Descent presents as a good alternative for Batch Gradient Descent, mainly because of its computation efficiency. In the future experiments, we will refer to the family of _vanilla_ Gradient Descent algorithms as SGD with variable batch size.\n",
    "\n",
    "While SGD is effective in finding good sets of weights that minimize error, it turns out that Gradient Descent can further be improved by introducing more dynamic mechanism in adjusting of the model's weights. Think of SGD as _velocity_ in a world governed by classical Physics. If there's _velocity_, then there should also be _acceleration_, _momentum_, and such!\n",
    "\n",
    "In this notebook, we'll implement __1st Moment__, __RMSprop__ and __Adam__ optimization algorithms. These algorithms describe _dynamic_ variations of SGD, and are considered to be improvement of the _dry_ SGD. In particular __RMSProp and Adam__ have been shown to work well across a wide range of deep learning architectures. \n",
    "\n",
    "* Adam: https://arxiv.org/abs/1412.6980\n",
    "* RMSProp: https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified data/t10k-images-idx3-ubyte.gz\n",
      "Found and verified data/t10k-labels-idx1-ubyte.gz\n",
      "Found and verified data/train-images-idx3-ubyte.gz\n",
      "Found and verified data/train-labels-idx1-ubyte.gz\n",
      "Found and verified data/t10k-images-idx3-ubyte.gz\n",
      "Found and verified data/t10k-labels-idx1-ubyte.gz\n",
      "Found and verified data/train-images-idx3-ubyte.gz\n",
      "Found and verified data/train-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist\n",
    "from activation import relu, sigmoid, sigmoid_prime, softmax\n",
    "from helper import one_hot_encoder\n",
    "from initializer import initialize_weight\n",
    "%matplotlib inline\n",
    "\n",
    "from utils import dataloader\n",
    "from losses import cross_entropy_loss\n",
    "\n",
    "# Load Dataset\n",
    "train_x, train_y = mnist.load_dataset(download=True, train=True)\n",
    "test_x, test_y = mnist.load_dataset(download=True, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Moment\n",
    "\n",
    "Here are the equations of weight updates using moment:\n",
    "\n",
    "Keypoints of optimization using moments:\n",
    "\n",
    "1. Moment takes into account the __history of gradient values__. This helps the algorithm in choosing whether to speed up or slow down the gradient descent, and therefore, smoothing the optimization.\n",
    "2. Usual values of beta ranges from __0.8 to 0.999__. The most common value of beta is __0.9__.\n",
    "3. One way of checking if your implementation of moment is correct is by setting the beta value to 0. Setting the beta value to 0 is the same as performing normal gradient descent.\n",
    "4. __Larger to normal values of learning rate should be used__ when using 1st Moment optimization.\n",
    "\n",
    "#### Tunable Hyperparameters:\n",
    "* beta_1\n",
    "* learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_moment(train_x, train_y, learning_rate=0.01, num_epochs=50, batch_size=None):\n",
    "    # Flatten input (num_samples, 28, 28) -> (num_samples, 784) \n",
    "    x = train_x.reshape(train_x.shape[0], -1)\n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # Turn labels into their one-hot representations\n",
    "    y = one_hot_encoder(train_y)\n",
    "\n",
    "    # Make a data loader\n",
    "    trainloader = dataloader(x, y, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w1, b1 = initialize_weight((784, 256), bias=True)\n",
    "    w2, b2 = initialize_weight((256, 10), bias=True)\n",
    "\n",
    "    # Initialize Moments\n",
    "    v_w1, v_b1 = np.zeros(w1.shape), np.zeros(b1.shape)\n",
    "    v_w2, v_b2 = np.zeros(w2.shape), np.zeros(b2.shape)\n",
    "    \n",
    "    # Optimizer Hyperparameters\n",
    "    beta_1 = 0.9\n",
    "    \n",
    "    loss_history = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(\"Epoch {}/{}\\n===============\".format(epoch, num_epochs))\n",
    "\n",
    "        batch_loss = 0\n",
    "        acc = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            # Number of samples per batch\n",
    "            m = inputs.shape[0]\n",
    "            \n",
    "            # Forward Prop\n",
    "            h1 = np.dot(inputs, w1) + b1\n",
    "            a1 = sigmoid(h1)\n",
    "            h2 = np.dot(a1, w2) + b2\n",
    "            a2 = softmax(h2)\n",
    "            out = a2\n",
    "\n",
    "            # Cross Entropy Loss\n",
    "            batch_loss += cross_entropy_loss(out, labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Compute Accuracy\n",
    "            pred = np.argmax(out, axis=1)\n",
    "            pred = pred.reshape(pred.shape[0], 1)\n",
    "            acc += np.sum(pred == labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Backward Prop\n",
    "            dh2 = a2 - labels \n",
    "            dw2 = (1/m) * np.dot(a1.T, dh2)\n",
    "            db2 = (1/m) * np.sum(dh2, axis=0, keepdims=True)\n",
    "\n",
    "            dh1 = np.dot(dh2, w2.T) * sigmoid_prime(a1)\n",
    "            dw1 = (1/m) * np.dot(inputs.T, dh1)\n",
    "            db1 = (1/m) * np.sum(dh1, axis=0, keepdims=True)\n",
    "\n",
    "            # 1st Moment\n",
    "            v_w2 = beta_1 * v_w2 + (1-beta_1) * dw2\n",
    "            v_b2 = beta_1 * v_b2 + (1-beta_1) * db2\n",
    "            v_w1 = beta_1 * v_w1 + (1-beta_1) * dw1\n",
    "            v_b1 = beta_1 * v_b1 + (1-beta_1) * db1\n",
    "            \n",
    "            # Weight (and bias) update\n",
    "            w1 -= learning_rate * v_w1\n",
    "            b1 -= learning_rate * v_b1\n",
    "            w2 -= learning_rate * v_w2\n",
    "            b2 -= learning_rate * v_b2\n",
    "            \n",
    "        loss_history.append(batch_loss/num_samples)\n",
    "        print(\"Loss: {:.6f}\".format(batch_loss/num_samples))\n",
    "        print(\"Accuracy: {:.2f}%\\n\".format(acc/num_samples*100))\n",
    "\n",
    "    return w1, b1, w2, b2, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "===============\n",
      "Loss: 0.679610\n",
      "Accuracy: 83.38%\n",
      "\n",
      "Epoch 2/20\n",
      "===============\n",
      "Loss: 0.342920\n",
      "Accuracy: 90.96%\n",
      "\n",
      "Epoch 3/20\n",
      "===============\n",
      "Loss: 0.281672\n",
      "Accuracy: 92.36%\n",
      "\n",
      "Epoch 4/20\n",
      "===============\n",
      "Loss: 0.247413\n",
      "Accuracy: 93.29%\n",
      "\n",
      "Epoch 5/20\n",
      "===============\n",
      "Loss: 0.223667\n",
      "Accuracy: 93.92%\n",
      "\n",
      "Epoch 6/20\n",
      "===============\n",
      "Loss: 0.204176\n",
      "Accuracy: 94.44%\n",
      "\n",
      "Epoch 7/20\n",
      "===============\n",
      "Loss: 0.191056\n",
      "Accuracy: 94.80%\n",
      "\n",
      "Epoch 8/20\n",
      "===============\n",
      "Loss: 0.176864\n",
      "Accuracy: 95.11%\n",
      "\n",
      "Epoch 9/20\n",
      "===============\n",
      "Loss: 0.168828\n",
      "Accuracy: 95.35%\n",
      "\n",
      "Epoch 10/20\n",
      "===============\n",
      "Loss: 0.160342\n",
      "Accuracy: 95.62%\n",
      "\n",
      "Epoch 11/20\n",
      "===============\n",
      "Loss: 0.151357\n",
      "Accuracy: 95.83%\n",
      "\n",
      "Epoch 12/20\n",
      "===============\n",
      "Loss: 0.144298\n",
      "Accuracy: 96.04%\n",
      "\n",
      "Epoch 13/20\n",
      "===============\n",
      "Loss: 0.137218\n",
      "Accuracy: 96.19%\n",
      "\n",
      "Epoch 14/20\n",
      "===============\n",
      "Loss: 0.131595\n",
      "Accuracy: 96.45%\n",
      "\n",
      "Epoch 15/20\n",
      "===============\n",
      "Loss: 0.127305\n",
      "Accuracy: 96.53%\n",
      "\n",
      "Epoch 16/20\n",
      "===============\n",
      "Loss: 0.122842\n",
      "Accuracy: 96.73%\n",
      "\n",
      "Epoch 17/20\n",
      "===============\n",
      "Loss: 0.119617\n",
      "Accuracy: 96.76%\n",
      "\n",
      "Epoch 18/20\n",
      "===============\n",
      "Loss: 0.115598\n",
      "Accuracy: 96.86%\n",
      "\n",
      "Epoch 19/20\n",
      "===============\n",
      "Loss: 0.108998\n",
      "Accuracy: 97.06%\n",
      "\n",
      "Epoch 20/20\n",
      "===============\n",
      "Loss: 0.106863\n",
      "Accuracy: 97.11%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1_m, b1_m, w2_m, b2_m, loss_history_m = train_moment(train_x, train_y, learning_rate=0.01, num_epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp: 2nd Moment\n",
    "\n",
    "Here are the equations of RMSProp:\n",
    "\n",
    "Keypoints:\n",
    "\n",
    "1. Usual values of __2nd moment beta is 0.999__.\n",
    "2. A small number (epsilon) is added to the denominator to avoid division by zero. Typically, the value of epsilon is 1e-8, and this does not need any tuning.\n",
    "3. __Smaller values of learning rate should be used__ when using RMSProp optimization.\n",
    "\n",
    "#### Tunable Hyperparameters:\n",
    "* beta_2\n",
    "* learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rmsprop(train_x, train_y, learning_rate=0.0001, num_epochs=50, batch_size=None):\n",
    "    # Flatten input (num_samples, 28, 28) -> (num_samples, 784) \n",
    "    x = train_x.reshape(train_x.shape[0], -1)\n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # Turn labels into their one-hot representations\n",
    "    y = one_hot_encoder(train_y)\n",
    "\n",
    "    # Make a data loader\n",
    "    trainloader = dataloader(x, y, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w1, b1 = initialize_weight((784, 256), bias=True)\n",
    "    w2, b2 = initialize_weight((256, 10), bias=True)\n",
    "\n",
    "    # Initialize Moments\n",
    "    s_w1, s_b1 = np.zeros(w1.shape), np.zeros(b1.shape)\n",
    "    s_w2, s_b2 = np.zeros(w2.shape), np.zeros(b2.shape)\n",
    "    \n",
    "    # Optimizer Hyperparameters\n",
    "    beta_2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    loss_history = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(\"Epoch {}/{}\\n===============\".format(epoch, num_epochs))\n",
    "\n",
    "        batch_loss = 0\n",
    "        acc = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            # Number of samples per batch\n",
    "            m = inputs.shape[0]\n",
    "            \n",
    "            # Forward Prop\n",
    "            h1 = np.dot(inputs, w1) + b1\n",
    "            a1 = sigmoid(h1)\n",
    "            h2 = np.dot(a1, w2) + b2\n",
    "            a2 = softmax(h2)\n",
    "            out = a2\n",
    "\n",
    "            # Cross Entropy Loss\n",
    "            batch_loss += cross_entropy_loss(out, labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Compute Accuracy\n",
    "            pred = np.argmax(out, axis=1)\n",
    "            pred = pred.reshape(pred.shape[0], 1)\n",
    "            acc += np.sum(pred == labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Backward Prop\n",
    "            dh2 = a2 - labels \n",
    "            dw2 = (1/m) * np.dot(a1.T, dh2)\n",
    "            db2 = (1/m) * np.sum(dh2, axis=0, keepdims=True)\n",
    "\n",
    "            dh1 = np.dot(dh2, w2.T) * sigmoid_prime(a1)\n",
    "            dw1 = (1/m) * np.dot(inputs.T, dh1)\n",
    "            db1 = (1/m) * np.sum(dh1, axis=0, keepdims=True)\n",
    "            \n",
    "            # 2nd Moment\n",
    "            s_w2 = beta_2 * s_w2 + (1-beta_2) * dw2 * dw2\n",
    "            s_b2 = beta_2 * s_b2 + (1-beta_2) * db2 * db2\n",
    "            s_w1 = beta_2 * s_w1 + (1-beta_2) * dw1 * dw1\n",
    "            s_b1 = beta_2 * s_b1 + (1-beta_2) * db1 * db1\n",
    "            \n",
    "            # Weight (and bias) update\n",
    "            w1 -= learning_rate * dw1 / (np.sqrt(s_w1) + epsilon)\n",
    "            b1 -= learning_rate * db1 / (np.sqrt(s_b1) + epsilon)\n",
    "            w2 -= learning_rate * dw2 / (np.sqrt(s_w2) + epsilon)\n",
    "            b2 -= learning_rate * db2 / (np.sqrt(s_b2) + epsilon)\n",
    "            \n",
    "        loss_history.append(batch_loss/num_samples)\n",
    "        print(\"Loss: {:.6f}\".format(batch_loss/num_samples))\n",
    "        print(\"Accuracy: {:.2f}%\\n\".format(acc/num_samples*100))\n",
    "\n",
    "    return w1, b1, w2, b2, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "===============\n",
      "Loss: 0.503672\n",
      "Accuracy: 87.60%\n",
      "\n",
      "Epoch 2/20\n",
      "===============\n",
      "Loss: 0.296200\n",
      "Accuracy: 92.02%\n",
      "\n",
      "Epoch 3/20\n",
      "===============\n",
      "Loss: 0.246794\n",
      "Accuracy: 93.23%\n",
      "\n",
      "Epoch 4/20\n",
      "===============\n",
      "Loss: 0.219212\n",
      "Accuracy: 93.90%\n",
      "\n",
      "Epoch 5/20\n",
      "===============\n",
      "Loss: 0.198185\n",
      "Accuracy: 94.44%\n",
      "\n",
      "Epoch 6/20\n",
      "===============\n",
      "Loss: 0.181241\n",
      "Accuracy: 94.89%\n",
      "\n",
      "Epoch 7/20\n",
      "===============\n",
      "Loss: 0.168127\n",
      "Accuracy: 95.27%\n",
      "\n",
      "Epoch 8/20\n",
      "===============\n",
      "Loss: 0.157311\n",
      "Accuracy: 95.53%\n",
      "\n",
      "Epoch 9/20\n",
      "===============\n",
      "Loss: 0.147967\n",
      "Accuracy: 95.84%\n",
      "\n",
      "Epoch 10/20\n",
      "===============\n",
      "Loss: 0.138592\n",
      "Accuracy: 96.12%\n",
      "\n",
      "Epoch 11/20\n",
      "===============\n",
      "Loss: 0.130903\n",
      "Accuracy: 96.34%\n",
      "\n",
      "Epoch 12/20\n",
      "===============\n",
      "Loss: 0.124156\n",
      "Accuracy: 96.51%\n",
      "\n",
      "Epoch 13/20\n",
      "===============\n",
      "Loss: 0.116830\n",
      "Accuracy: 96.68%\n",
      "\n",
      "Epoch 14/20\n",
      "===============\n",
      "Loss: 0.111475\n",
      "Accuracy: 96.88%\n",
      "\n",
      "Epoch 15/20\n",
      "===============\n",
      "Loss: 0.107178\n",
      "Accuracy: 97.04%\n",
      "\n",
      "Epoch 16/20\n",
      "===============\n",
      "Loss: 0.100845\n",
      "Accuracy: 97.19%\n",
      "\n",
      "Epoch 17/20\n",
      "===============\n",
      "Loss: 0.096173\n",
      "Accuracy: 97.34%\n",
      "\n",
      "Epoch 18/20\n",
      "===============\n",
      "Loss: 0.092696\n",
      "Accuracy: 97.41%\n",
      "\n",
      "Epoch 19/20\n",
      "===============\n",
      "Loss: 0.088006\n",
      "Accuracy: 97.61%\n",
      "\n",
      "Epoch 20/20\n",
      "===============\n",
      "Loss: 0.085557\n",
      "Accuracy: 97.63%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1_r, b1_r, w2_r, b2_r, loss_history_r = train_rmsprop(train_x, train_y, learning_rate=0.0001, num_epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam: Moment + RMSProp\n",
    "\n",
    "Here are the equation for Adam algorithm:\n",
    "\n",
    "Keypoints:\n",
    "\n",
    "1. Usual values of 1st moment beta ranges from __0.8 to 0.999__. The most common value of beta is __0.9__.\n",
    "2. Usual value of 2nd moment beta is __0.999__. \n",
    "3. Similar to RMSProp, a small number (epsilon) with a value of 1e-8 is added to the denominator to avoid math errors. \n",
    "4. Use normal values of learning_rate, in between 1st Moment's and RMSProp's. Whatever learning rate works for SGD, should also work for Adam.\n",
    "\n",
    "#### Tunable Hyperparameters\n",
    "* beta_1\n",
    "* beta_2\n",
    "* learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adam(train_x, train_y, learning_rate=0.001, num_epochs=50, batch_size=None):\n",
    "    # Flatten input (num_samples, 28, 28) -> (num_samples, 784) \n",
    "    x = train_x.reshape(train_x.shape[0], -1)\n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # Turn labels into their one-hot representations\n",
    "    y = one_hot_encoder(train_y)\n",
    "\n",
    "    # Make a data loader\n",
    "    trainloader = dataloader(x, y, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w1, b1 = initialize_weight((784, 256), bias=True)\n",
    "    w2, b2 = initialize_weight((256, 10), bias=True)\n",
    "\n",
    "    # Initialize Moments\n",
    "    s_w1, s_b1 = np.zeros(w1.shape), np.zeros(b1.shape)\n",
    "    s_w2, s_b2 = np.zeros(w2.shape), np.zeros(b2.shape)\n",
    "    v_w1, v_b1 = np.zeros(w1.shape), np.zeros(b1.shape)\n",
    "    v_w2, v_b2 = np.zeros(w2.shape), np.zeros(b2.shape)\n",
    "    \n",
    "    # Optimizer Hyperparameters\n",
    "    beta_1 = 0.9\n",
    "    beta_2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    loss_history = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(\"Epoch {}/{}\\n===============\".format(epoch, num_epochs))\n",
    "\n",
    "        batch_loss = 0\n",
    "        acc = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            # Number of samples per batch\n",
    "            m = inputs.shape[0]\n",
    "            \n",
    "            # Forward Prop\n",
    "            h1 = np.dot(inputs, w1) + b1\n",
    "            a1 = sigmoid(h1)\n",
    "            h2 = np.dot(a1, w2) + b2\n",
    "            a2 = softmax(h2)\n",
    "            out = a2\n",
    "\n",
    "            # Cross Entropy Loss\n",
    "            batch_loss += cross_entropy_loss(out, labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Compute Accuracy\n",
    "            pred = np.argmax(out, axis=1)\n",
    "            pred = pred.reshape(pred.shape[0], 1)\n",
    "            acc += np.sum(pred == labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Backward Prop\n",
    "            dh2 = a2 - labels \n",
    "            dw2 = (1/m) * np.dot(a1.T, dh2)\n",
    "            db2 = (1/m) * np.sum(dh2, axis=0, keepdims=True)\n",
    "\n",
    "            dh1 = np.dot(dh2, w2.T) * sigmoid_prime(a1)\n",
    "            dw1 = (1/m) * np.dot(inputs.T, dh1)\n",
    "            db1 = (1/m) * np.sum(dh1, axis=0, keepdims=True)\n",
    "\n",
    "            # 1st Moment\n",
    "            v_w2 = beta_1 * v_w2 + (1-beta_1) * dw2\n",
    "            v_b2 = beta_1 * v_b2 + (1-beta_1) * db2\n",
    "            v_w1 = beta_1 * v_w1 + (1-beta_1) * dw1\n",
    "            v_b1 = beta_1 * v_b1 + (1-beta_1) * db1\n",
    "            \n",
    "            # 2nd Moment\n",
    "            s_w2 = beta_2 * s_w2 + (1-beta_2) * dw2 * dw2\n",
    "            s_b2 = beta_2 * s_b2 + (1-beta_2) * db2 * db2\n",
    "            s_w1 = beta_2 * s_w1 + (1-beta_2) * dw1 * dw1\n",
    "            s_b1 = beta_2 * s_b1 + (1-beta_2) * db1 * db1\n",
    "            \n",
    "            # Weight (and bias) update\n",
    "            w1 -= learning_rate * v_w1 / (np.sqrt(s_w1) + epsilon)\n",
    "            b1 -= learning_rate * v_b1 / (np.sqrt(s_b1) + epsilon)\n",
    "            w2 -= learning_rate * v_w2 / (np.sqrt(s_w2) + epsilon)\n",
    "            b2 -= learning_rate * v_b2 / (np.sqrt(s_b2) + epsilon)\n",
    "            \n",
    "        loss_history.append(batch_loss/num_samples)\n",
    "        print(\"Loss: {:.6f}\".format(batch_loss/num_samples))\n",
    "        print(\"Accuracy: {:.2f}%\\n\".format(acc/num_samples*100))\n",
    "\n",
    "    return w1, b1, w2, b2, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "===============\n",
      "Loss: 0.450903\n",
      "Accuracy: 88.27%\n",
      "\n",
      "Epoch 2/20\n",
      "===============\n",
      "Loss: 0.258609\n",
      "Accuracy: 92.80%\n",
      "\n",
      "Epoch 3/20\n",
      "===============\n",
      "Loss: 0.220338\n",
      "Accuracy: 93.78%\n",
      "\n",
      "Epoch 4/20\n",
      "===============\n",
      "Loss: 0.189961\n",
      "Accuracy: 94.55%\n",
      "\n",
      "Epoch 5/20\n",
      "===============\n",
      "Loss: 0.175986\n",
      "Accuracy: 94.99%\n",
      "\n",
      "Epoch 6/20\n",
      "===============\n",
      "Loss: 0.160359\n",
      "Accuracy: 95.43%\n",
      "\n",
      "Epoch 7/20\n",
      "===============\n",
      "Loss: 0.148074\n",
      "Accuracy: 95.74%\n",
      "\n",
      "Epoch 8/20\n",
      "===============\n",
      "Loss: 0.137862\n",
      "Accuracy: 95.97%\n",
      "\n",
      "Epoch 9/20\n",
      "===============\n",
      "Loss: 0.131753\n",
      "Accuracy: 96.22%\n",
      "\n",
      "Epoch 10/20\n",
      "===============\n",
      "Loss: 0.126759\n",
      "Accuracy: 96.33%\n",
      "\n",
      "Epoch 11/20\n",
      "===============\n",
      "Loss: 0.119713\n",
      "Accuracy: 96.51%\n",
      "\n",
      "Epoch 12/20\n",
      "===============\n",
      "Loss: 0.113012\n",
      "Accuracy: 96.70%\n",
      "\n",
      "Epoch 13/20\n",
      "===============\n",
      "Loss: 0.107655\n",
      "Accuracy: 96.89%\n",
      "\n",
      "Epoch 14/20\n",
      "===============\n",
      "Loss: 0.105729\n",
      "Accuracy: 96.87%\n",
      "\n",
      "Epoch 15/20\n",
      "===============\n",
      "Loss: 0.099434\n",
      "Accuracy: 97.11%\n",
      "\n",
      "Epoch 16/20\n",
      "===============\n",
      "Loss: 0.098018\n",
      "Accuracy: 97.12%\n",
      "\n",
      "Epoch 17/20\n",
      "===============\n",
      "Loss: 0.095806\n",
      "Accuracy: 97.21%\n",
      "\n",
      "Epoch 18/20\n",
      "===============\n",
      "Loss: 0.092298\n",
      "Accuracy: 97.30%\n",
      "\n",
      "Epoch 19/20\n",
      "===============\n",
      "Loss: 0.086160\n",
      "Accuracy: 97.51%\n",
      "\n",
      "Epoch 20/20\n",
      "===============\n",
      "Loss: 0.085433\n",
      "Accuracy: 97.56%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1_a, b1_a, w2_a, b2_a, loss_history_a = train_adam(train_x, train_y, learning_rate=0.0002, num_epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_x, train_y, learning_rate=0.1, num_epochs=50, batch_size=1):\n",
    "    # Flatten input (num_samples, 28, 28) -> (num_samples, 784) \n",
    "    x = train_x.reshape(train_x.shape[0], -1)\n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # Turn labels into their one-hot representations\n",
    "    y = one_hot_encoder(train_y)\n",
    "\n",
    "    # Make a data loader\n",
    "    trainloader = dataloader(x, y, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w1, b1 = initialize_weight((784, 256), bias=True)\n",
    "    w2, b2 = initialize_weight((256, 10), bias=True)\n",
    "\n",
    "    loss_history = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(\"Epoch {}/{}\\n===============\".format(epoch, num_epochs))\n",
    "\n",
    "        batch_loss = 0\n",
    "        acc = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            # Number of samples per batch\n",
    "            m = inputs.shape[0]\n",
    "            \n",
    "            # Forward Prop\n",
    "            h1 = np.dot(inputs, w1) + b1\n",
    "            a1 = sigmoid(h1)\n",
    "            h2 = np.dot(a1, w2) + b2\n",
    "            a2 = softmax(h2)\n",
    "            out = a2\n",
    "\n",
    "            # Cross Entropy Loss\n",
    "            batch_loss += cross_entropy_loss(out, labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Compute Accuracy\n",
    "            pred = np.argmax(out, axis=1)\n",
    "            pred = pred.reshape(pred.shape[0], 1)\n",
    "            acc += np.sum(pred == labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Backward Prop\n",
    "            dh2 = a2 - labels \n",
    "            dw2 = (1/m) * np.dot(a1.T, dh2)\n",
    "            db2 = (1/m) * np.sum(dh2, axis=0, keepdims=True)\n",
    "\n",
    "            dh1 = np.dot(dh2, w2.T) * sigmoid_prime(a1)\n",
    "            dw1 = (1/m) * np.dot(inputs.T, dh1)\n",
    "            db1 = (1/m) * np.sum(dh1, axis=0, keepdims=True)\n",
    "\n",
    "            # Weight (and bias) update\n",
    "            w1 -= learning_rate * dw1\n",
    "            b1 -= learning_rate * db1\n",
    "            w2 -= learning_rate * dw2\n",
    "            b2 -= learning_rate * db2\n",
    "            \n",
    "        loss_history.append(batch_loss/num_samples)\n",
    "        print(\"Loss: {:.6f}\".format(batch_loss/num_samples))\n",
    "        print(\"Accuracy: {:.2f}%\\n\".format(acc/num_samples*100))\n",
    "\n",
    "    return w1, b1, w2, b2, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "===============\n",
      "Loss: 0.862686\n",
      "Accuracy: 79.84%\n",
      "\n",
      "Epoch 2/20\n",
      "===============\n",
      "Loss: 0.425613\n",
      "Accuracy: 89.46%\n",
      "\n",
      "Epoch 3/20\n",
      "===============\n",
      "Loss: 0.339804\n",
      "Accuracy: 91.18%\n",
      "\n",
      "Epoch 4/20\n",
      "===============\n",
      "Loss: 0.295572\n",
      "Accuracy: 92.16%\n",
      "\n",
      "Epoch 5/20\n",
      "===============\n",
      "Loss: 0.265906\n",
      "Accuracy: 92.87%\n",
      "\n",
      "Epoch 6/20\n",
      "===============\n",
      "Loss: 0.244839\n",
      "Accuracy: 93.41%\n",
      "\n",
      "Epoch 7/20\n",
      "===============\n",
      "Loss: 0.225978\n",
      "Accuracy: 93.92%\n",
      "\n",
      "Epoch 8/20\n",
      "===============\n",
      "Loss: 0.212800\n",
      "Accuracy: 94.23%\n",
      "\n",
      "Epoch 9/20\n",
      "===============\n",
      "Loss: 0.201151\n",
      "Accuracy: 94.55%\n",
      "\n",
      "Epoch 10/20\n",
      "===============\n",
      "Loss: 0.189974\n",
      "Accuracy: 94.86%\n",
      "\n",
      "Epoch 11/20\n",
      "===============\n",
      "Loss: 0.180166\n",
      "Accuracy: 95.04%\n",
      "\n",
      "Epoch 12/20\n",
      "===============\n",
      "Loss: 0.172098\n",
      "Accuracy: 95.37%\n",
      "\n",
      "Epoch 13/20\n",
      "===============\n",
      "Loss: 0.164900\n",
      "Accuracy: 95.58%\n",
      "\n",
      "Epoch 14/20\n",
      "===============\n",
      "Loss: 0.157727\n",
      "Accuracy: 95.85%\n",
      "\n",
      "Epoch 15/20\n",
      "===============\n",
      "Loss: 0.151278\n",
      "Accuracy: 95.95%\n",
      "\n",
      "Epoch 16/20\n",
      "===============\n",
      "Loss: 0.146804\n",
      "Accuracy: 96.14%\n",
      "\n",
      "Epoch 17/20\n",
      "===============\n",
      "Loss: 0.141740\n",
      "Accuracy: 96.26%\n",
      "\n",
      "Epoch 18/20\n",
      "===============\n",
      "Loss: 0.135459\n",
      "Accuracy: 96.48%\n",
      "\n",
      "Epoch 19/20\n",
      "===============\n",
      "Loss: 0.130371\n",
      "Accuracy: 96.62%\n",
      "\n",
      "Epoch 20/20\n",
      "===============\n",
      "Loss: 0.126228\n",
      "Accuracy: 96.76%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1_mbgd, b1_mbgd, w2_mbgd, b2_mbgd, loss_history_mbgd = train(train_x, train_y, learning_rate=0.005, num_epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1822b264d30>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot and Compare Losses\n",
    "plt.plot(loss_history_mbgd, label=\"vanilla SGD\")\n",
    "plt.plot(loss_history_m, label=\"Moment\")\n",
    "plt.plot(loss_history_r, label=\"RMSProp\")\n",
    "plt.plot(loss_history_a, label=\"Adam\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "So let's analyze the graph above. As expected based on the discussions above, the 3 new optimization algorithms outperform SGD. Adam performed slightly better than RMSProp. Notice that these optimizers are highly affected by the choice of learning rate. \n",
    "\n",
    "### Learning Rate Choice\n",
    "Listed below is the trend of the values of effective learning rates, with the top requiring the largest largest value:\n",
    "1. Moment\n",
    "2. Vanilla SGD\n",
    "3. Adam\n",
    "4. RMSProp\n",
    "\n",
    "\n",
    "### Power-Performance Trade-off \n",
    "Also, take note that vanilla SGD requires the least computational power, but it performs the worst in terms of loss and accuracy. We are therefore presented with _computation power vs performance trade off_. With increasing performance potential, more computation power is needed.\n",
    "\n",
    "Trend of decreasing performance:\n",
    "1. Adam\n",
    "2. RMSProp\n",
    "3. Moment\n",
    "4. vanilla SGD\n",
    "\n",
    "Trend of decreasing computation power needed: \n",
    "1. vanilla SGD,\n",
    "2. Moment\n",
    "3. RMSProp\n",
    "4. Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
