{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithms\n",
    "\n",
    "In the previous experiment, we have explored some variations of Gradient Descent, particularly, __mini-Batch Gradient Descent (mBGD)__, and __Stochastic Gradient Descent (SGD)__. Mini-batch Gradient Descent presents as a good alternative for Batch Gradient Descent, mainly because of its computation efficiency. In the future experiments, we will refer to the family of _vanilla_ Gradient Descent algorithms as SGD with variable batch size.\n",
    "\n",
    "While SGD is effective in finding good sets of weights that minimize error, it turns out that Gradient Descent can further be improved by introducing more dynamic mechanism in adjusting of the model's weights. Think of SGD as _velocity_ in a world governed by classical Physics. If there's _velocity_, then there should also be _acceleration_, _momentum_, and such!\n",
    "\n",
    "In this notebook, we'll implement __1st Moment__, __RMSprop__ and __Adam__ optimization algorithms. These algorithms describe _dynamic_ variations of SGD, and are considered to be improvement of the _dry_ SGD. In particular __RMSProp and Adam__ have been shown to work well across a wide range of deep learning architectures. \n",
    "\n",
    "* Adam: https://arxiv.org/abs/1412.6980\n",
    "* RMSProp: https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified data/t10k-images-idx3-ubyte.gz\n",
      "Found and verified data/t10k-labels-idx1-ubyte.gz\n",
      "Found and verified data/train-images-idx3-ubyte.gz\n",
      "Found and verified data/train-labels-idx1-ubyte.gz\n",
      "Found and verified data/t10k-images-idx3-ubyte.gz\n",
      "Found and verified data/t10k-labels-idx1-ubyte.gz\n",
      "Found and verified data/train-images-idx3-ubyte.gz\n",
      "Found and verified data/train-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist\n",
    "from activation import relu, sigmoid, sigmoid_prime, softmax\n",
    "from helper import one_hot_encoder\n",
    "from initializer import initialize_weight\n",
    "%matplotlib inline\n",
    "\n",
    "from utils import dataloader\n",
    "from losses import cross_entropy_loss\n",
    "\n",
    "# Load Dataset\n",
    "train_x, train_y = mnist.load_dataset(download=True, train=True)\n",
    "test_x, test_y = mnist.load_dataset(download=True, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Moment\n",
    "\n",
    "Here are the equations of weight updates using moment:\n",
    "\n",
    "Keypoints of optimization using moments:\n",
    "\n",
    "1. Moment takes into account the __history of gradient values__. This helps the algorithm in choosing whether to speed up or slow down the gradient descent, and therefore, smoothing the optimization.\n",
    "2. Usual values of beta ranges from __0.8 to 0.999__. The most common value of beta is __0.9__.\n",
    "3. One way of checking if your implementation of moment is correct is by setting the beta value to 0. Setting the beta value to 0 is the same as performing normal gradient descent.\n",
    "4. __Larger to normal values of learning rate should be used__ when using 1st Moment optimization.\n",
    "\n",
    "#### Tunable Hyperparameters:\n",
    "* beta_1\n",
    "* learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_moment(train_x, train_y, learning_rate=0.1, num_epochs=50, batch_size=None):\n",
    "    # Flatten input (num_samples, 28, 28) -> (num_samples, 784) \n",
    "    x = train_x.reshape(train_x.shape[0], -1)\n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # Turn labels into their one-hot representations\n",
    "    y = one_hot_encoder(train_y)\n",
    "\n",
    "    # Make a data loader\n",
    "    trainloader = dataloader(x, y, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w1, b1 = initialize_weight((784, 256), bias=True)\n",
    "    w2, b2 = initialize_weight((256, 10), bias=True)\n",
    "\n",
    "    # Initialize Moments\n",
    "    v_w1, v_b1 = np.zeros(w1.shape), np.zeros(b1.shape)\n",
    "    v_w2, v_b2 = np.zeros(w2.shape), np.zeros(b2.shape)\n",
    "    \n",
    "    # Optimizer Hyperparameters\n",
    "    beta_1 = 0.9\n",
    "    \n",
    "    loss_history = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(\"Epoch {}/{}\\n===============\".format(epoch, num_epochs))\n",
    "\n",
    "        batch_loss = 0\n",
    "        acc = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            # Number of samples per batch\n",
    "            m = inputs.shape[0]\n",
    "            \n",
    "            # Forward Prop\n",
    "            h1 = np.dot(inputs, w1) + b1\n",
    "            a1 = sigmoid(h1)\n",
    "            h2 = np.dot(a1, w2) + b2\n",
    "            a2 = softmax(h2)\n",
    "            out = a2\n",
    "\n",
    "            # Cross Entropy Loss\n",
    "            batch_loss += cross_entropy_loss(out, labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Compute Accuracy\n",
    "            pred = np.argmax(out, axis=1)\n",
    "            pred = pred.reshape(pred.shape[0], 1)\n",
    "            acc += np.sum(pred == labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Backward Prop\n",
    "            dh2 = a2 - labels \n",
    "            dw2 = (1/m) * np.dot(a1.T, dh2)\n",
    "            db2 = (1/m) * np.sum(dh2, axis=0, keepdims=True)\n",
    "\n",
    "            dh1 = np.dot(dh2, w2.T) * sigmoid_prime(a1)\n",
    "            dw1 = (1/m) * np.dot(inputs.T, dh1)\n",
    "            db1 = (1/m) * np.sum(dh1, axis=0, keepdims=True)\n",
    "\n",
    "            # 1st Moment\n",
    "            v_w2 = beta_1 * v_w2 + (1-beta_1) * dw2\n",
    "            v_b2 = beta_1 * v_b2 + (1-beta_1) * db2\n",
    "            v_w1 = beta_1 * v_w1 + (1-beta_1) * dw1\n",
    "            v_b1 = beta_1 * v_b1 + (1-beta_1) * db1\n",
    "            \n",
    "            # Weight (and bias) update\n",
    "            w1 -= learning_rate * v_w1\n",
    "            b1 -= learning_rate * v_b1\n",
    "            w2 -= learning_rate * v_w2\n",
    "            b2 -= learning_rate * v_b2\n",
    "            \n",
    "        loss_history.append(batch_loss/num_samples)\n",
    "        print(\"Loss: {:.6f}\".format(batch_loss/num_samples))\n",
    "        print(\"Accuracy: {:.2f}%\\n\".format(acc/num_samples*100))\n",
    "\n",
    "    return w1, b1, w2, b2, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "===============\n",
      "Loss: 0.679610\n",
      "Accuracy: 83.38%\n",
      "\n",
      "Epoch 2/20\n",
      "===============\n",
      "Loss: 0.342920\n",
      "Accuracy: 90.96%\n",
      "\n",
      "Epoch 3/20\n",
      "===============\n",
      "Loss: 0.281672\n",
      "Accuracy: 92.36%\n",
      "\n",
      "Epoch 4/20\n",
      "===============\n",
      "Loss: 0.247413\n",
      "Accuracy: 93.29%\n",
      "\n",
      "Epoch 5/20\n",
      "===============\n",
      "Loss: 0.223667\n",
      "Accuracy: 93.92%\n",
      "\n",
      "Epoch 6/20\n",
      "===============\n",
      "Loss: 0.204176\n",
      "Accuracy: 94.44%\n",
      "\n",
      "Epoch 7/20\n",
      "===============\n",
      "Loss: 0.191056\n",
      "Accuracy: 94.80%\n",
      "\n",
      "Epoch 8/20\n",
      "===============\n",
      "Loss: 0.176864\n",
      "Accuracy: 95.11%\n",
      "\n",
      "Epoch 9/20\n",
      "===============\n",
      "Loss: 0.168828\n",
      "Accuracy: 95.35%\n",
      "\n",
      "Epoch 10/20\n",
      "===============\n",
      "Loss: 0.160342\n",
      "Accuracy: 95.62%\n",
      "\n",
      "Epoch 11/20\n",
      "===============\n",
      "Loss: 0.151357\n",
      "Accuracy: 95.83%\n",
      "\n",
      "Epoch 12/20\n",
      "===============\n",
      "Loss: 0.144298\n",
      "Accuracy: 96.04%\n",
      "\n",
      "Epoch 13/20\n",
      "===============\n",
      "Loss: 0.137218\n",
      "Accuracy: 96.19%\n",
      "\n",
      "Epoch 14/20\n",
      "===============\n",
      "Loss: 0.131595\n",
      "Accuracy: 96.45%\n",
      "\n",
      "Epoch 15/20\n",
      "===============\n",
      "Loss: 0.127305\n",
      "Accuracy: 96.53%\n",
      "\n",
      "Epoch 16/20\n",
      "===============\n",
      "Loss: 0.122842\n",
      "Accuracy: 96.73%\n",
      "\n",
      "Epoch 17/20\n",
      "===============\n",
      "Loss: 0.119617\n",
      "Accuracy: 96.76%\n",
      "\n",
      "Epoch 18/20\n",
      "===============\n",
      "Loss: 0.115598\n",
      "Accuracy: 96.86%\n",
      "\n",
      "Epoch 19/20\n",
      "===============\n",
      "Loss: 0.108998\n",
      "Accuracy: 97.06%\n",
      "\n",
      "Epoch 20/20\n",
      "===============\n",
      "Loss: 0.106863\n",
      "Accuracy: 97.11%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1_m, b1_m, w2_m, b2_m, loss_history_m = train_moment(train_x, train_y, learning_rate=0.01, num_epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp: 2nd Moment\n",
    "\n",
    "Here are the equations of RMSProp:\n",
    "\n",
    "Keypoints:\n",
    "\n",
    "1. Usual values of __2nd moment beta is 0.999__.\n",
    "2. A small number (epsilon) is added to the denominator to avoid division by zero. Typically, the value of epsilon is 1e-8, and this does not need any tuning.\n",
    "3. __Smaller values of learning rate should be used__ when using RMSProp optimization.\n",
    "\n",
    "#### Tunable Hyperparameters:\n",
    "* beta_2\n",
    "* learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rmsprop(train_x, train_y, learning_rate=0.1, num_epochs=50, batch_size=None):\n",
    "    # Flatten input (num_samples, 28, 28) -> (num_samples, 784) \n",
    "    x = train_x.reshape(train_x.shape[0], -1)\n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # Turn labels into their one-hot representations\n",
    "    y = one_hot_encoder(train_y)\n",
    "\n",
    "    # Make a data loader\n",
    "    trainloader = dataloader(x, y, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w1, b1 = initialize_weight((784, 256), bias=True)\n",
    "    w2, b2 = initialize_weight((256, 10), bias=True)\n",
    "\n",
    "    # Initialize Moments\n",
    "    s_w1, s_b1 = np.zeros(w1.shape), np.zeros(b1.shape)\n",
    "    s_w2, s_b2 = np.zeros(w2.shape), np.zeros(b2.shape)\n",
    "    \n",
    "    # Optimizer Hyperparameters\n",
    "    beta_2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    loss_history = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(\"Epoch {}/{}\\n===============\".format(epoch, num_epochs))\n",
    "\n",
    "        batch_loss = 0\n",
    "        acc = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            # Number of samples per batch\n",
    "            m = inputs.shape[0]\n",
    "            \n",
    "            # Forward Prop\n",
    "            h1 = np.dot(inputs, w1) + b1\n",
    "            a1 = sigmoid(h1)\n",
    "            h2 = np.dot(a1, w2) + b2\n",
    "            a2 = softmax(h2)\n",
    "            out = a2\n",
    "\n",
    "            # Cross Entropy Loss\n",
    "            batch_loss += cross_entropy_loss(out, labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Compute Accuracy\n",
    "            pred = np.argmax(out, axis=1)\n",
    "            pred = pred.reshape(pred.shape[0], 1)\n",
    "            acc += np.sum(pred == labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Backward Prop\n",
    "            dh2 = a2 - labels \n",
    "            dw2 = (1/m) * np.dot(a1.T, dh2)\n",
    "            db2 = (1/m) * np.sum(dh2, axis=0, keepdims=True)\n",
    "\n",
    "            dh1 = np.dot(dh2, w2.T) * sigmoid_prime(a1)\n",
    "            dw1 = (1/m) * np.dot(inputs.T, dh1)\n",
    "            db1 = (1/m) * np.sum(dh1, axis=0, keepdims=True)\n",
    "            \n",
    "            # 2nd Moment\n",
    "            s_w2 = beta_2 * s_w2 + (1-beta_2) * dw2 * dw2\n",
    "            s_b2 = beta_2 * s_b2 + (1-beta_2) * db2 * db2\n",
    "            s_w1 = beta_2 * s_w1 + (1-beta_2) * dw1 * dw1\n",
    "            s_b1 = beta_2 * s_b1 + (1-beta_2) * db1 * db1\n",
    "            \n",
    "            # Weight (and bias) update\n",
    "            w1 -= learning_rate * dw1 / (np.sqrt(s_w1) + epsilon)\n",
    "            b1 -= learning_rate * db1 / (np.sqrt(s_b1) + epsilon)\n",
    "            w2 -= learning_rate * dw2 / (np.sqrt(s_w2) + epsilon)\n",
    "            b2 -= learning_rate * db2 / (np.sqrt(s_b2) + epsilon)\n",
    "            \n",
    "        loss_history.append(batch_loss/num_samples)\n",
    "        print(\"Loss: {:.6f}\".format(batch_loss/num_samples))\n",
    "        print(\"Accuracy: {:.2f}%\\n\".format(acc/num_samples*100))\n",
    "\n",
    "    return w1, b1, w2, b2, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "===============\n",
      "Loss: 0.503672\n",
      "Accuracy: 87.60%\n",
      "\n",
      "Epoch 2/20\n",
      "===============\n",
      "Loss: 0.296200\n",
      "Accuracy: 92.02%\n",
      "\n",
      "Epoch 3/20\n",
      "===============\n",
      "Loss: 0.246794\n",
      "Accuracy: 93.23%\n",
      "\n",
      "Epoch 4/20\n",
      "===============\n",
      "Loss: 0.219212\n",
      "Accuracy: 93.90%\n",
      "\n",
      "Epoch 5/20\n",
      "===============\n",
      "Loss: 0.198185\n",
      "Accuracy: 94.44%\n",
      "\n",
      "Epoch 6/20\n",
      "===============\n",
      "Loss: 0.181241\n",
      "Accuracy: 94.89%\n",
      "\n",
      "Epoch 7/20\n",
      "===============\n",
      "Loss: 0.168127\n",
      "Accuracy: 95.27%\n",
      "\n",
      "Epoch 8/20\n",
      "===============\n",
      "Loss: 0.157311\n",
      "Accuracy: 95.53%\n",
      "\n",
      "Epoch 9/20\n",
      "===============\n",
      "Loss: 0.147967\n",
      "Accuracy: 95.84%\n",
      "\n",
      "Epoch 10/20\n",
      "===============\n",
      "Loss: 0.138592\n",
      "Accuracy: 96.12%\n",
      "\n",
      "Epoch 11/20\n",
      "===============\n",
      "Loss: 0.130903\n",
      "Accuracy: 96.34%\n",
      "\n",
      "Epoch 12/20\n",
      "===============\n",
      "Loss: 0.124156\n",
      "Accuracy: 96.51%\n",
      "\n",
      "Epoch 13/20\n",
      "===============\n",
      "Loss: 0.116830\n",
      "Accuracy: 96.68%\n",
      "\n",
      "Epoch 14/20\n",
      "===============\n",
      "Loss: 0.111475\n",
      "Accuracy: 96.88%\n",
      "\n",
      "Epoch 15/20\n",
      "===============\n",
      "Loss: 0.107178\n",
      "Accuracy: 97.04%\n",
      "\n",
      "Epoch 16/20\n",
      "===============\n",
      "Loss: 0.100845\n",
      "Accuracy: 97.19%\n",
      "\n",
      "Epoch 17/20\n",
      "===============\n",
      "Loss: 0.096173\n",
      "Accuracy: 97.34%\n",
      "\n",
      "Epoch 18/20\n",
      "===============\n",
      "Loss: 0.092696\n",
      "Accuracy: 97.41%\n",
      "\n",
      "Epoch 19/20\n",
      "===============\n",
      "Loss: 0.088006\n",
      "Accuracy: 97.61%\n",
      "\n",
      "Epoch 20/20\n",
      "===============\n",
      "Loss: 0.085557\n",
      "Accuracy: 97.63%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1_r, b1_r, w2_r, b2_r, loss_history_r = train_rmsprop(train_x, train_y, learning_rate=0.0001, num_epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam: Moment + RMSProp\n",
    "\n",
    "Here are the equation for Adam algorithm:\n",
    "\n",
    "Keypoints:\n",
    "\n",
    "1. Usual values of 1st moment beta ranges from __0.8 to 0.999__. The most common value of beta is __0.9__.\n",
    "2. Usual value of 2nd moment beta is __0.999__. \n",
    "3. Similar to RMSProp, a small number (epsilon) with a value of 1e-8 is added to the denominator to avoid math errors. \n",
    "4. Use normal values of learning_rate, in between 1st Moment's and RMSProp's. Whatever learning rate works for SGD, should also work for Adam.\n",
    "\n",
    "#### Tunable Hyperparameters\n",
    "* beta_1\n",
    "* beta_2\n",
    "* learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adam(train_x, train_y, learning_rate=0.1, num_epochs=50, batch_size=None):\n",
    "    # Flatten input (num_samples, 28, 28) -> (num_samples, 784) \n",
    "    x = train_x.reshape(train_x.shape[0], -1)\n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # Turn labels into their one-hot representations\n",
    "    y = one_hot_encoder(train_y)\n",
    "\n",
    "    # Make a data loader\n",
    "    trainloader = dataloader(x, y, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w1, b1 = initialize_weight((784, 256), bias=True)\n",
    "    w2, b2 = initialize_weight((256, 10), bias=True)\n",
    "\n",
    "    # Initialize Moments\n",
    "    s_w1, s_b1 = np.zeros(w1.shape), np.zeros(b1.shape)\n",
    "    s_w2, s_b2 = np.zeros(w2.shape), np.zeros(b2.shape)\n",
    "    v_w1, v_b1 = np.zeros(w1.shape), np.zeros(b1.shape)\n",
    "    v_w2, v_b2 = np.zeros(w2.shape), np.zeros(b2.shape)\n",
    "    \n",
    "    # Optimizer Hyperparameters\n",
    "    beta_1 = 0.9\n",
    "    beta_2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    loss_history = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(\"Epoch {}/{}\\n===============\".format(epoch, num_epochs))\n",
    "\n",
    "        batch_loss = 0\n",
    "        acc = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            # Number of samples per batch\n",
    "            m = inputs.shape[0]\n",
    "            \n",
    "            # Forward Prop\n",
    "            h1 = np.dot(inputs, w1) + b1\n",
    "            a1 = sigmoid(h1)\n",
    "            h2 = np.dot(a1, w2) + b2\n",
    "            a2 = softmax(h2)\n",
    "            out = a2\n",
    "\n",
    "            # Cross Entropy Loss\n",
    "            batch_loss += cross_entropy_loss(out, labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Compute Accuracy\n",
    "            pred = np.argmax(out, axis=1)\n",
    "            pred = pred.reshape(pred.shape[0], 1)\n",
    "            acc += np.sum(pred == labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Backward Prop\n",
    "            dh2 = a2 - labels \n",
    "            dw2 = (1/m) * np.dot(a1.T, dh2)\n",
    "            db2 = (1/m) * np.sum(dh2, axis=0, keepdims=True)\n",
    "\n",
    "            dh1 = np.dot(dh2, w2.T) * sigmoid_prime(a1)\n",
    "            dw1 = (1/m) * np.dot(inputs.T, dh1)\n",
    "            db1 = (1/m) * np.sum(dh1, axis=0, keepdims=True)\n",
    "\n",
    "            # 1st Moment\n",
    "            v_w2 = (beta_1 * v_w2 + (1-beta_1) * dw2) * (1-beta_1)\n",
    "            v_b2 = (beta_1 * v_b2 + (1-beta_1) * db2) * (1-beta_1)\n",
    "            v_w1 = (beta_1 * v_w1 + (1-beta_1) * dw1) * (1-beta_1)\n",
    "            v_b1 = (beta_1 * v_b1 + (1-beta_1) * db1) * (1-beta_1)\n",
    "            \n",
    "            # 2nd Moment\n",
    "            s_w2 = beta_2 * s_w2 + (1-beta_2) * dw2 * dw2\n",
    "            s_b2 = beta_2 * s_b2 + (1-beta_2) * db2 * db2\n",
    "            s_w1 = beta_2 * s_w1 + (1-beta_2) * dw1 * dw1\n",
    "            s_b1 = beta_2 * s_b1 + (1-beta_2) * db1 * db1\n",
    "            \n",
    "            # Weight (and bias) update\n",
    "            w1 -= learning_rate * v_w1 / (np.sqrt(s_w1) + epsilon)\n",
    "            b1 -= learning_rate * v_b1 / (np.sqrt(s_b1) + epsilon)\n",
    "            w2 -= learning_rate * v_w2 / (np.sqrt(s_w2) + epsilon)\n",
    "            b2 -= learning_rate * v_b2 / (np.sqrt(s_b2) + epsilon)\n",
    "            \n",
    "        loss_history.append(batch_loss/num_samples)\n",
    "        print(\"Loss: {:.6f}\".format(batch_loss/num_samples))\n",
    "        print(\"Accuracy: {:.2f}%\\n\".format(acc/num_samples*100))\n",
    "\n",
    "    return w1, b1, w2, b2, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "===============\n",
      "Loss: 0.479834\n",
      "Accuracy: 88.11%\n",
      "\n",
      "Epoch 2/20\n",
      "===============\n",
      "Loss: 0.285150\n",
      "Accuracy: 92.29%\n",
      "\n",
      "Epoch 3/20\n",
      "===============\n",
      "Loss: 0.239493\n",
      "Accuracy: 93.35%\n",
      "\n",
      "Epoch 4/20\n",
      "===============\n",
      "Loss: 0.210518\n",
      "Accuracy: 94.06%\n",
      "\n",
      "Epoch 5/20\n",
      "===============\n",
      "Loss: 0.191409\n",
      "Accuracy: 94.63%\n",
      "\n",
      "Epoch 6/20\n",
      "===============\n",
      "Loss: 0.176133\n",
      "Accuracy: 95.05%\n",
      "\n",
      "Epoch 7/20\n",
      "===============\n",
      "Loss: 0.163253\n",
      "Accuracy: 95.43%\n",
      "\n",
      "Epoch 8/20\n",
      "===============\n",
      "Loss: 0.151851\n",
      "Accuracy: 95.71%\n",
      "\n",
      "Epoch 9/20\n",
      "===============\n",
      "Loss: 0.141485\n",
      "Accuracy: 96.08%\n",
      "\n",
      "Epoch 10/20\n",
      "===============\n",
      "Loss: 0.132993\n",
      "Accuracy: 96.25%\n",
      "\n",
      "Epoch 11/20\n",
      "===============\n",
      "Loss: 0.126351\n",
      "Accuracy: 96.47%\n",
      "\n",
      "Epoch 12/20\n",
      "===============\n",
      "Loss: 0.118287\n",
      "Accuracy: 96.71%\n",
      "\n",
      "Epoch 13/20\n",
      "===============\n",
      "Loss: 0.112748\n",
      "Accuracy: 96.93%\n",
      "\n",
      "Epoch 14/20\n",
      "===============\n",
      "Loss: 0.106196\n",
      "Accuracy: 97.00%\n",
      "\n",
      "Epoch 15/20\n",
      "===============\n",
      "Loss: 0.103137\n",
      "Accuracy: 97.14%\n",
      "\n",
      "Epoch 16/20\n",
      "===============\n",
      "Loss: 0.098680\n",
      "Accuracy: 97.26%\n",
      "\n",
      "Epoch 17/20\n",
      "===============\n",
      "Loss: 0.094111\n",
      "Accuracy: 97.40%\n",
      "\n",
      "Epoch 18/20\n",
      "===============\n",
      "Loss: 0.089501\n",
      "Accuracy: 97.58%\n",
      "\n",
      "Epoch 19/20\n",
      "===============\n",
      "Loss: 0.085754\n",
      "Accuracy: 97.64%\n",
      "\n",
      "Epoch 20/20\n",
      "===============\n",
      "Loss: 0.082753\n",
      "Accuracy: 97.69%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1_a, b1_a, w2_a, b2_a, loss_history_a = train_adam(train_x, train_y, learning_rate=0.01, num_epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_x, train_y, learning_rate=0.1, num_epochs=50, batch_size=1):\n",
    "    # Flatten input (num_samples, 28, 28) -> (num_samples, 784) \n",
    "    x = train_x.reshape(train_x.shape[0], -1)\n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # Turn labels into their one-hot representations\n",
    "    y = one_hot_encoder(train_y)\n",
    "\n",
    "    # Make a data loader\n",
    "    trainloader = dataloader(x, y, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w1, b1 = initialize_weight((784, 256), bias=True)\n",
    "    w2, b2 = initialize_weight((256, 10), bias=True)\n",
    "\n",
    "    loss_history = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(\"Epoch {}/{}\\n===============\".format(epoch, num_epochs))\n",
    "\n",
    "        batch_loss = 0\n",
    "        acc = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            # Number of samples per batch\n",
    "            m = inputs.shape[0]\n",
    "            \n",
    "            # Forward Prop\n",
    "            h1 = np.dot(inputs, w1) + b1\n",
    "            a1 = sigmoid(h1)\n",
    "            h2 = np.dot(a1, w2) + b2\n",
    "            a2 = softmax(h2)\n",
    "            out = a2\n",
    "\n",
    "            # Cross Entropy Loss\n",
    "            batch_loss += cross_entropy_loss(out, labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Compute Accuracy\n",
    "            pred = np.argmax(out, axis=1)\n",
    "            pred = pred.reshape(pred.shape[0], 1)\n",
    "            acc += np.sum(pred == labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Backward Prop\n",
    "            dh2 = a2 - labels \n",
    "            dw2 = (1/m) * np.dot(a1.T, dh2)\n",
    "            db2 = (1/m) * np.sum(dh2, axis=0, keepdims=True)\n",
    "\n",
    "            dh1 = np.dot(dh2, w2.T) * sigmoid_prime(a1)\n",
    "            dw1 = (1/m) * np.dot(inputs.T, dh1)\n",
    "            db1 = (1/m) * np.sum(dh1, axis=0, keepdims=True)\n",
    "\n",
    "            # Weight (and bias) update\n",
    "            w1 -= learning_rate * dw1\n",
    "            b1 -= learning_rate * db1\n",
    "            w2 -= learning_rate * dw2\n",
    "            b2 -= learning_rate * db2\n",
    "            \n",
    "        loss_history.append(batch_loss/num_samples)\n",
    "        print(\"Loss: {:.6f}\".format(batch_loss/num_samples))\n",
    "        print(\"Accuracy: {:.2f}%\\n\".format(acc/num_samples*100))\n",
    "\n",
    "    return w1, b1, w2, b2, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "===============\n",
      "Loss: 0.862686\n",
      "Accuracy: 79.84%\n",
      "\n",
      "Epoch 2/20\n",
      "===============\n",
      "Loss: 0.425613\n",
      "Accuracy: 89.46%\n",
      "\n",
      "Epoch 3/20\n",
      "===============\n",
      "Loss: 0.339804\n",
      "Accuracy: 91.18%\n",
      "\n",
      "Epoch 4/20\n",
      "===============\n",
      "Loss: 0.295572\n",
      "Accuracy: 92.16%\n",
      "\n",
      "Epoch 5/20\n",
      "===============\n",
      "Loss: 0.265906\n",
      "Accuracy: 92.87%\n",
      "\n",
      "Epoch 6/20\n",
      "===============\n",
      "Loss: 0.244839\n",
      "Accuracy: 93.41%\n",
      "\n",
      "Epoch 7/20\n",
      "===============\n",
      "Loss: 0.225978\n",
      "Accuracy: 93.92%\n",
      "\n",
      "Epoch 8/20\n",
      "===============\n",
      "Loss: 0.212800\n",
      "Accuracy: 94.23%\n",
      "\n",
      "Epoch 9/20\n",
      "===============\n",
      "Loss: 0.201151\n",
      "Accuracy: 94.55%\n",
      "\n",
      "Epoch 10/20\n",
      "===============\n",
      "Loss: 0.189974\n",
      "Accuracy: 94.86%\n",
      "\n",
      "Epoch 11/20\n",
      "===============\n",
      "Loss: 0.180166\n",
      "Accuracy: 95.04%\n",
      "\n",
      "Epoch 12/20\n",
      "===============\n",
      "Loss: 0.172098\n",
      "Accuracy: 95.37%\n",
      "\n",
      "Epoch 13/20\n",
      "===============\n",
      "Loss: 0.164900\n",
      "Accuracy: 95.58%\n",
      "\n",
      "Epoch 14/20\n",
      "===============\n",
      "Loss: 0.157727\n",
      "Accuracy: 95.85%\n",
      "\n",
      "Epoch 15/20\n",
      "===============\n",
      "Loss: 0.151278\n",
      "Accuracy: 95.95%\n",
      "\n",
      "Epoch 16/20\n",
      "===============\n",
      "Loss: 0.146804\n",
      "Accuracy: 96.14%\n",
      "\n",
      "Epoch 17/20\n",
      "===============\n",
      "Loss: 0.141740\n",
      "Accuracy: 96.26%\n",
      "\n",
      "Epoch 18/20\n",
      "===============\n",
      "Loss: 0.135459\n",
      "Accuracy: 96.48%\n",
      "\n",
      "Epoch 19/20\n",
      "===============\n",
      "Loss: 0.130371\n",
      "Accuracy: 96.62%\n",
      "\n",
      "Epoch 20/20\n",
      "===============\n",
      "Loss: 0.126228\n",
      "Accuracy: 96.76%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1_mbgd, b1_mbgd, w2_mbgd, b2_mbgd, loss_history_mbgd = train(train_x, train_y, learning_rate=0.005, num_epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1822a677eb8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot and Compare Losses\n",
    "plt.plot(loss_history_mbgd, label=\"vanilla SGD\")\n",
    "plt.plot(loss_history_m, label=\"Moment\")\n",
    "plt.plot(loss_history_r, label=\"RMSProp\")\n",
    "plt.plot(loss_history_a, label=\"Adam\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "So let's analyze the graph above. As expected based on the discussions above, the 3 new optimization algorithms outperform SGD. Adam performed slightly better than RMSProp. Notice that these optimizers are highly affected by the choice of learning rate. \n",
    "\n",
    "### Learning Rate Choice\n",
    "Listed below is the trend of the values of effective learning rates, with the top requiring the largest largest value:\n",
    "1. Moment\n",
    "2. Adam\n",
    "3. Vanilla SGD\n",
    "4. RMSProp\n",
    "\n",
    "\n",
    "### Power-Performance Trade-off \n",
    "Also, take note that vanilla SGD requires the least computational power, but it performs the worst in terms of loss and accuracy. We are therefore presented with _computation power vs performance trade off_. With increasing performance potential, more computation power is needed.\n",
    "\n",
    "Trend of decreasing performance:\n",
    "1. Adam\n",
    "2. RMSProp\n",
    "3. Moment\n",
    "4. vanilla SGD\n",
    "\n",
    "Trend of decreasing computation power needed: \n",
    "1. vanilla SGD,\n",
    "2. Moment\n",
    "3. RMSProp\n",
    "4. Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
