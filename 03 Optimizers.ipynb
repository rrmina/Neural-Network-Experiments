{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithms\n",
    "\n",
    "In the previous experiment, we have explored some variations of Gradient Descent, particularly, __mini-Batch Gradient Descent (mBGD)__, and __Stochastic Gradient Descent (SGD)__. Mini-batch Gradient Descent presents as a good alternative for Batch Gradient Descent, mainly because of its computation efficiency. In the future experiments, we will refer to the family of _vanilla_ Gradient Descent algorithms as SGD with variable batch size.\n",
    "\n",
    "While SGD is effective in finding good sets of weights that minimize error, it turns out that Gradient Descent can further be improved by introducing more dynamic mechanism in adjusting of the model's weights. Think of SGD as _velocity_ in a world governed by classical Physics. If there's _velocity_, then there should also be _acceleration_, _momentum_, and such!\n",
    "\n",
    "In this notebook, we'll implement __1st Moment__, __RMSprop__ and __Adam__ optimization algorithms. These algorithms describe _dynamic_ variations of SGD, and are considered to be improvement of the _dry_ SGD. In particular __RMSProp and Adam__ have been shown to work well across a wide range of deep learning architectures. \n",
    "\n",
    "* Adam: https://arxiv.org/abs/1412.6980\n",
    "* RMSProp: https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified data/t10k-images-idx3-ubyte.gz\n",
      "Found and verified data/t10k-labels-idx1-ubyte.gz\n",
      "Found and verified data/train-images-idx3-ubyte.gz\n",
      "Found and verified data/train-labels-idx1-ubyte.gz\n",
      "Found and verified data/t10k-images-idx3-ubyte.gz\n",
      "Found and verified data/t10k-labels-idx1-ubyte.gz\n",
      "Found and verified data/train-images-idx3-ubyte.gz\n",
      "Found and verified data/train-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist\n",
    "from activation import relu, sigmoid, sigmoid_prime, softmax\n",
    "from helper import one_hot_encoder\n",
    "from initializer import initialize_weight\n",
    "%matplotlib inline\n",
    "\n",
    "from utils import dataloader\n",
    "from losses import cross_entropy_loss\n",
    "\n",
    "# Load Dataset\n",
    "train_x, train_y = mnist.load_dataset(download=True, train=True)\n",
    "test_x, test_y = mnist.load_dataset(download=True, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Moment\n",
    "\n",
    "Here are the equations of weight updates using moment:\n",
    "\n",
    "Keypoints of optimization using moments:\n",
    "\n",
    "1. Moment takes into account the __history of gradient values__. This helps the algorithm in choosing whether to speed up or slow down the gradient descent, and therefore, smoothing the optimization.\n",
    "2. Usual values of beta ranges from __0.8 to 0.999__. The most common value of beta is __0.9__.\n",
    "3. One way of checking if your implementation of moment is correct is by setting the beta value to 0. Setting the beta value to 0 is the same as performing normal gradient descent.\n",
    "4. __Larger values of learning rate should be used__ when using 1st Moment optimization.\n",
    "\n",
    "#### Tunable Hyperparameters:\n",
    "* beta_1\n",
    "* learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_moment(train_x, train_y, learning_rate=0.1, num_epochs=50, batch_size=None):\n",
    "    # Flatten input (num_samples, 28, 28) -> (num_samples, 784) \n",
    "    x = train_x.reshape(train_x.shape[0], -1)\n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # Turn labels into their one-hot representations\n",
    "    y = one_hot_encoder(train_y)\n",
    "\n",
    "    # Make a data loader\n",
    "    trainloader = dataloader(x, y, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w1, b1 = initialize_weight((784, 256), bias=True)\n",
    "    w2, b2 = initialize_weight((256, 10), bias=True)\n",
    "\n",
    "    # Initialize Moments\n",
    "    v_w1, v_b1 = np.zeros(w1.shape), np.zeros(b1.shape)\n",
    "    v_w2, v_b2 = np.zeros(w2.shape), np.zeros(b2.shape)\n",
    "    \n",
    "    # Optimizer Hyperparameters\n",
    "    beta_1 = 0.9\n",
    "    \n",
    "    loss_history = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(\"Epoch {}/{}\\n===============\".format(epoch, num_epochs))\n",
    "\n",
    "        batch_loss = 0\n",
    "        acc = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            # Number of samples per batch\n",
    "            m = inputs.shape[0]\n",
    "            \n",
    "            # Forward Prop\n",
    "            h1 = np.dot(inputs, w1) + b1\n",
    "            a1 = sigmoid(h1)\n",
    "            h2 = np.dot(a1, w2) + b2\n",
    "            a2 = softmax(h2)\n",
    "            out = a2\n",
    "\n",
    "            # Cross Entropy Loss\n",
    "            batch_loss += cross_entropy_loss(out, labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Compute Accuracy\n",
    "            pred = np.argmax(out, axis=1)\n",
    "            pred = pred.reshape(pred.shape[0], 1)\n",
    "            acc += np.sum(pred == labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Backward Prop\n",
    "            dh2 = a2 - labels \n",
    "            dw2 = (1/m) * np.dot(a1.T, dh2)\n",
    "            db2 = (1/m) * np.sum(dh2, axis=0, keepdims=True)\n",
    "\n",
    "            dh1 = np.dot(dh2, w2.T) * sigmoid_prime(a1)\n",
    "            dw1 = (1/m) * np.dot(inputs.T, dh1)\n",
    "            db1 = (1/m) * np.sum(dh1, axis=0, keepdims=True)\n",
    "\n",
    "            # 1st Moment\n",
    "            v_w2 = beta_1 * v_w2 + (1-beta_1) * dw2\n",
    "            v_b2 = beta_1 * v_b2 + (1-beta_1) * db2\n",
    "            v_w1 = beta_1 * v_w1 + (1-beta_1) * dw1\n",
    "            v_b1 = beta_1 * v_b1 + (1-beta_1) * db1\n",
    "            \n",
    "            # Weight (and bias) update\n",
    "            w1 -= learning_rate * v_w1\n",
    "            b1 -= learning_rate * v_b1\n",
    "            w2 -= learning_rate * v_w2\n",
    "            b2 -= learning_rate * v_b2\n",
    "            \n",
    "        loss_history.append(batch_loss/num_samples)\n",
    "        print(\"Loss: {:.6f}\".format(batch_loss/num_samples))\n",
    "        print(\"Accuracy: {:.2f}%\\n\".format(acc/num_samples*100))\n",
    "\n",
    "    return w1, b1, w2, b2, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "===============\n",
      "Loss: 0.547895\n",
      "Accuracy: 85.16%\n",
      "\n",
      "Epoch 2/20\n",
      "===============\n",
      "Loss: 0.407795\n",
      "Accuracy: 88.68%\n",
      "\n",
      "Epoch 3/20\n",
      "===============\n",
      "Loss: 0.395177\n",
      "Accuracy: 88.65%\n",
      "\n",
      "Epoch 4/20\n",
      "===============\n",
      "Loss: 0.389133\n",
      "Accuracy: 88.78%\n",
      "\n",
      "Epoch 5/20\n",
      "===============\n",
      "Loss: 0.388719\n",
      "Accuracy: 88.66%\n",
      "\n",
      "Epoch 6/20\n",
      "===============\n",
      "Loss: 0.375823\n",
      "Accuracy: 88.97%\n",
      "\n",
      "Epoch 7/20\n",
      "===============\n",
      "Loss: 0.377468\n",
      "Accuracy: 88.67%\n",
      "\n",
      "Epoch 8/20\n",
      "===============\n",
      "Loss: 0.379442\n",
      "Accuracy: 88.57%\n",
      "\n",
      "Epoch 9/20\n",
      "===============\n",
      "Loss: 0.375957\n",
      "Accuracy: 88.77%\n",
      "\n",
      "Epoch 10/20\n",
      "===============\n",
      "Loss: 0.358250\n",
      "Accuracy: 89.15%\n",
      "\n",
      "Epoch 11/20\n",
      "===============\n",
      "Loss: 0.356646\n",
      "Accuracy: 89.30%\n",
      "\n",
      "Epoch 12/20\n",
      "===============\n",
      "Loss: 0.337675\n",
      "Accuracy: 89.93%\n",
      "\n",
      "Epoch 13/20\n",
      "===============\n",
      "Loss: 0.358345\n",
      "Accuracy: 88.98%\n",
      "\n",
      "Epoch 14/20\n",
      "===============\n",
      "Loss: 0.361158\n",
      "Accuracy: 89.02%\n",
      "\n",
      "Epoch 15/20\n",
      "===============\n",
      "Loss: 0.341011\n",
      "Accuracy: 89.78%\n",
      "\n",
      "Epoch 16/20\n",
      "===============\n",
      "Loss: 0.326716\n",
      "Accuracy: 89.94%\n",
      "\n",
      "Epoch 17/20\n",
      "===============\n",
      "Loss: 0.330672\n",
      "Accuracy: 89.83%\n",
      "\n",
      "Epoch 18/20\n",
      "===============\n",
      "Loss: 0.337413\n",
      "Accuracy: 89.61%\n",
      "\n",
      "Epoch 19/20\n",
      "===============\n",
      "Loss: 0.321611\n",
      "Accuracy: 90.28%\n",
      "\n",
      "Epoch 20/20\n",
      "===============\n",
      "Loss: 0.321972\n",
      "Accuracy: 90.22%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1_m, b1_m, w2_m, b2_m, loss_history_m = train_moment(train_x, train_y, learning_rate=0.1, num_epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp: 2nd Moment\n",
    "\n",
    "Here are the equations of RMSProp:\n",
    "\n",
    "Keypoints:\n",
    "\n",
    "1. Usual values of __2nd moment beta is 0.999__.\n",
    "2. A small number (epsilon) is added to the denominator to avoid division by zero. Typically, the value of epsilon is 1e-8, and this does not need any tuning.\n",
    "3. __Smaller values of learning rate should be used__ when using RMSProp optimization.\n",
    "\n",
    "#### Tunable Hyperparameters:\n",
    "* beta_2\n",
    "* learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rmsprop(train_x, train_y, learning_rate=0.1, num_epochs=50, batch_size=None):\n",
    "    # Flatten input (num_samples, 28, 28) -> (num_samples, 784) \n",
    "    x = train_x.reshape(train_x.shape[0], -1)\n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # Turn labels into their one-hot representations\n",
    "    y = one_hot_encoder(train_y)\n",
    "\n",
    "    # Make a data loader\n",
    "    trainloader = dataloader(x, y, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w1, b1 = initialize_weight((784, 256), bias=True)\n",
    "    w2, b2 = initialize_weight((256, 10), bias=True)\n",
    "\n",
    "    # Initialize Moments\n",
    "    s_w1, s_b1 = np.zeros(w1.shape), np.zeros(b1.shape)\n",
    "    s_w2, s_b2 = np.zeros(w2.shape), np.zeros(b2.shape)\n",
    "    \n",
    "    # Optimizer Hyperparameters\n",
    "    beta_2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    loss_history = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(\"Epoch {}/{}\\n===============\".format(epoch, num_epochs))\n",
    "\n",
    "        batch_loss = 0\n",
    "        acc = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            # Number of samples per batch\n",
    "            m = inputs.shape[0]\n",
    "            \n",
    "            # Forward Prop\n",
    "            h1 = np.dot(inputs, w1) + b1\n",
    "            a1 = sigmoid(h1)\n",
    "            h2 = np.dot(a1, w2) + b2\n",
    "            a2 = softmax(h2)\n",
    "            out = a2\n",
    "\n",
    "            # Cross Entropy Loss\n",
    "            batch_loss += cross_entropy_loss(out, labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Compute Accuracy\n",
    "            pred = np.argmax(out, axis=1)\n",
    "            pred = pred.reshape(pred.shape[0], 1)\n",
    "            acc += np.sum(pred == labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Backward Prop\n",
    "            dh2 = a2 - labels \n",
    "            dw2 = (1/m) * np.dot(a1.T, dh2)\n",
    "            db2 = (1/m) * np.sum(dh2, axis=0, keepdims=True)\n",
    "\n",
    "            dh1 = np.dot(dh2, w2.T) * sigmoid_prime(a1)\n",
    "            dw1 = (1/m) * np.dot(inputs.T, dh1)\n",
    "            db1 = (1/m) * np.sum(dh1, axis=0, keepdims=True)\n",
    "            \n",
    "            # 2nd Moment\n",
    "            s_w2 = beta_2 * s_w2 + (1-beta_2) * dw2 * dw2\n",
    "            s_b2 = beta_2 * s_b2 + (1-beta_2) * db2 * db2\n",
    "            s_w1 = beta_2 * s_w1 + (1-beta_2) * dw1 * dw1\n",
    "            s_b1 = beta_2 * s_b1 + (1-beta_2) * db1 * db1\n",
    "            \n",
    "            # Weight (and bias) update\n",
    "            w1 -= learning_rate * dw1 / (np.sqrt(s_w1) + epsilon)\n",
    "            b1 -= learning_rate * db1 / (np.sqrt(s_b1) + epsilon)\n",
    "            w2 -= learning_rate * dw2 / (np.sqrt(s_w2) + epsilon)\n",
    "            b2 -= learning_rate * db2 / (np.sqrt(s_b2) + epsilon)\n",
    "            \n",
    "        loss_history.append(batch_loss/num_samples)\n",
    "        print(\"Loss: {:.6f}\".format(batch_loss/num_samples))\n",
    "        print(\"Accuracy: {:.2f}%\\n\".format(acc/num_samples*100))\n",
    "\n",
    "    return w1, b1, w2, b2, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "===============\n",
      "Loss: 0.543755\n",
      "Accuracy: 84.56%\n",
      "\n",
      "Epoch 2/20\n",
      "===============\n",
      "Loss: 0.319601\n",
      "Accuracy: 90.45%\n",
      "\n",
      "Epoch 3/20\n",
      "===============\n",
      "Loss: 0.287627\n",
      "Accuracy: 91.35%\n",
      "\n",
      "Epoch 4/20\n",
      "===============\n",
      "Loss: 0.267247\n",
      "Accuracy: 91.83%\n",
      "\n",
      "Epoch 5/20\n",
      "===============\n",
      "Loss: 0.253478\n",
      "Accuracy: 92.31%\n",
      "\n",
      "Epoch 6/20\n",
      "===============\n",
      "Loss: 0.242231\n",
      "Accuracy: 92.81%\n",
      "\n",
      "Epoch 7/20\n",
      "===============\n",
      "Loss: 0.234567\n",
      "Accuracy: 92.87%\n",
      "\n",
      "Epoch 8/20\n",
      "===============\n",
      "Loss: 0.224534\n",
      "Accuracy: 93.27%\n",
      "\n",
      "Epoch 9/20\n",
      "===============\n",
      "Loss: 0.227116\n",
      "Accuracy: 93.18%\n",
      "\n",
      "Epoch 10/20\n",
      "===============\n",
      "Loss: 0.208165\n",
      "Accuracy: 93.68%\n",
      "\n",
      "Epoch 11/20\n",
      "===============\n",
      "Loss: 0.208205\n",
      "Accuracy: 93.73%\n",
      "\n",
      "Epoch 12/20\n",
      "===============\n",
      "Loss: 0.208292\n",
      "Accuracy: 93.73%\n",
      "\n",
      "Epoch 13/20\n",
      "===============\n",
      "Loss: 0.196793\n",
      "Accuracy: 94.00%\n",
      "\n",
      "Epoch 14/20\n",
      "===============\n",
      "Loss: 0.190596\n",
      "Accuracy: 94.19%\n",
      "\n",
      "Epoch 15/20\n",
      "===============\n",
      "Loss: 0.188150\n",
      "Accuracy: 94.26%\n",
      "\n",
      "Epoch 16/20\n",
      "===============\n",
      "Loss: 0.188266\n",
      "Accuracy: 94.23%\n",
      "\n",
      "Epoch 17/20\n",
      "===============\n",
      "Loss: 0.187059\n",
      "Accuracy: 94.27%\n",
      "\n",
      "Epoch 18/20\n",
      "===============\n",
      "Loss: 0.178821\n",
      "Accuracy: 94.56%\n",
      "\n",
      "Epoch 19/20\n",
      "===============\n",
      "Loss: 0.179892\n",
      "Accuracy: 94.53%\n",
      "\n",
      "Epoch 20/20\n",
      "===============\n",
      "Loss: 0.177718\n",
      "Accuracy: 94.57%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1_r, b1_r, w2_r, b2_r, loss_history_r = train_rmsprop(train_x, train_y, learning_rate=0.001, num_epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam: Moment + RMSProp\n",
    "\n",
    "Here are the equation for Adam algorithm:\n",
    "\n",
    "Keypoints:\n",
    "\n",
    "1. Usual values of 1st moment beta ranges from __0.8 to 0.999__. The most common value of beta is __0.9__.\n",
    "2. Usual value of 2nd moment beta is __0.999__. \n",
    "3. Similar to RMSProp, a small number (epsilon) with a value of 1e-8 is added to the denominator to avoid math errors. \n",
    "4. Use normal values of learning_rate, in between 1st Moments and RMSProp.\n",
    "\n",
    "#### Tunable Hyperparameters\n",
    "* beta_1\n",
    "* beta_2\n",
    "* learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adam(train_x, train_y, learning_rate=0.1, num_epochs=50, batch_size=None):\n",
    "    # Flatten input (num_samples, 28, 28) -> (num_samples, 784) \n",
    "    x = train_x.reshape(train_x.shape[0], -1)\n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # Turn labels into their one-hot representations\n",
    "    y = one_hot_encoder(train_y)\n",
    "\n",
    "    # Make a data loader\n",
    "    trainloader = dataloader(x, y, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w1, b1 = initialize_weight((784, 256), bias=True)\n",
    "    w2, b2 = initialize_weight((256, 10), bias=True)\n",
    "\n",
    "    # Initialize Moments\n",
    "    s_w1, s_b1 = np.zeros(w1.shape), np.zeros(b1.shape)\n",
    "    s_w2, s_b2 = np.zeros(w2.shape), np.zeros(b2.shape)\n",
    "    v_w1, v_b1 = np.zeros(w1.shape), np.zeros(b1.shape)\n",
    "    v_w2, v_b2 = np.zeros(w2.shape), np.zeros(b2.shape)\n",
    "    \n",
    "    # Optimizer Hyperparameters\n",
    "    beta_1 = 0.9\n",
    "    beta_2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    loss_history = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(\"Epoch {}/{}\\n===============\".format(epoch, num_epochs))\n",
    "\n",
    "        batch_loss = 0\n",
    "        acc = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            # Number of samples per batch\n",
    "            m = inputs.shape[0]\n",
    "            \n",
    "            # Forward Prop\n",
    "            h1 = np.dot(inputs, w1) + b1\n",
    "            a1 = sigmoid(h1)\n",
    "            h2 = np.dot(a1, w2) + b2\n",
    "            a2 = softmax(h2)\n",
    "            out = a2\n",
    "\n",
    "            # Cross Entropy Loss\n",
    "            batch_loss += cross_entropy_loss(out, labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Compute Accuracy\n",
    "            pred = np.argmax(out, axis=1)\n",
    "            pred = pred.reshape(pred.shape[0], 1)\n",
    "            acc += np.sum(pred == labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Backward Prop\n",
    "            dh2 = a2 - labels \n",
    "            dw2 = (1/m) * np.dot(a1.T, dh2)\n",
    "            db2 = (1/m) * np.sum(dh2, axis=0, keepdims=True)\n",
    "\n",
    "            dh1 = np.dot(dh2, w2.T) * sigmoid_prime(a1)\n",
    "            dw1 = (1/m) * np.dot(inputs.T, dh1)\n",
    "            db1 = (1/m) * np.sum(dh1, axis=0, keepdims=True)\n",
    "\n",
    "            # 1st Moment\n",
    "            v_w2 = (beta_1 * v_w2 + (1-beta_1) * dw2) * (1-beta_1)\n",
    "            v_b2 = (beta_1 * v_b2 + (1-beta_1) * db2) * (1-beta_1)\n",
    "            v_w1 = (beta_1 * v_w1 + (1-beta_1) * dw1) * (1-beta_1)\n",
    "            v_b1 = (beta_1 * v_b1 + (1-beta_1) * db1) * (1-beta_1)\n",
    "            \n",
    "            # 2nd Moment\n",
    "            s_w2 = beta_2 * s_w2 + (1-beta_2) * dw2 * dw2\n",
    "            s_b2 = beta_2 * s_b2 + (1-beta_2) * db2 * db2\n",
    "            s_w1 = beta_2 * s_w1 + (1-beta_2) * dw1 * dw1\n",
    "            s_b1 = beta_2 * s_b1 + (1-beta_2) * db1 * db1\n",
    "            \n",
    "            # Weight (and bias) update\n",
    "            w1 -= learning_rate * v_w1 / (np.sqrt(s_w1) + epsilon)\n",
    "            b1 -= learning_rate * v_b1 / (np.sqrt(s_b1) + epsilon)\n",
    "            w2 -= learning_rate * v_w2 / (np.sqrt(s_w2) + epsilon)\n",
    "            b2 -= learning_rate * v_b2 / (np.sqrt(s_b2) + epsilon)\n",
    "            \n",
    "        loss_history.append(batch_loss/num_samples)\n",
    "        print(\"Loss: {:.6f}\".format(batch_loss/num_samples))\n",
    "        print(\"Accuracy: {:.2f}%\\n\".format(acc/num_samples*100))\n",
    "\n",
    "    return w1, b1, w2, b2, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "===============\n",
      "Loss: 0.479834\n",
      "Accuracy: 88.11%\n",
      "\n",
      "Epoch 2/20\n",
      "===============\n",
      "Loss: 0.285150\n",
      "Accuracy: 92.29%\n",
      "\n",
      "Epoch 3/20\n",
      "===============\n",
      "Loss: 0.239493\n",
      "Accuracy: 93.35%\n",
      "\n",
      "Epoch 4/20\n",
      "===============\n",
      "Loss: 0.210518\n",
      "Accuracy: 94.06%\n",
      "\n",
      "Epoch 5/20\n",
      "===============\n",
      "Loss: 0.191409\n",
      "Accuracy: 94.63%\n",
      "\n",
      "Epoch 6/20\n",
      "===============\n",
      "Loss: 0.176133\n",
      "Accuracy: 95.05%\n",
      "\n",
      "Epoch 7/20\n",
      "===============\n",
      "Loss: 0.163253\n",
      "Accuracy: 95.43%\n",
      "\n",
      "Epoch 8/20\n",
      "===============\n",
      "Loss: 0.151851\n",
      "Accuracy: 95.71%\n",
      "\n",
      "Epoch 9/20\n",
      "===============\n",
      "Loss: 0.141485\n",
      "Accuracy: 96.08%\n",
      "\n",
      "Epoch 10/20\n",
      "===============\n",
      "Loss: 0.132993\n",
      "Accuracy: 96.25%\n",
      "\n",
      "Epoch 11/20\n",
      "===============\n",
      "Loss: 0.126351\n",
      "Accuracy: 96.47%\n",
      "\n",
      "Epoch 12/20\n",
      "===============\n",
      "Loss: 0.118287\n",
      "Accuracy: 96.71%\n",
      "\n",
      "Epoch 13/20\n",
      "===============\n",
      "Loss: 0.112748\n",
      "Accuracy: 96.93%\n",
      "\n",
      "Epoch 14/20\n",
      "===============\n",
      "Loss: 0.106196\n",
      "Accuracy: 97.00%\n",
      "\n",
      "Epoch 15/20\n",
      "===============\n",
      "Loss: 0.103137\n",
      "Accuracy: 97.14%\n",
      "\n",
      "Epoch 16/20\n",
      "===============\n",
      "Loss: 0.098680\n",
      "Accuracy: 97.26%\n",
      "\n",
      "Epoch 17/20\n",
      "===============\n",
      "Loss: 0.094111\n",
      "Accuracy: 97.40%\n",
      "\n",
      "Epoch 18/20\n",
      "===============\n",
      "Loss: 0.089501\n",
      "Accuracy: 97.58%\n",
      "\n",
      "Epoch 19/20\n",
      "===============\n",
      "Loss: 0.085754\n",
      "Accuracy: 97.64%\n",
      "\n",
      "Epoch 20/20\n",
      "===============\n",
      "Loss: 0.082753\n",
      "Accuracy: 97.69%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1_a, b1_a, w2_a, b2_a, loss_history_a = train_adam(train_x, train_y, learning_rate=0.01, num_epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_x, train_y, learning_rate=0.1, num_epochs=50, batch_size=1):\n",
    "    # Flatten input (num_samples, 28, 28) -> (num_samples, 784) \n",
    "    x = train_x.reshape(train_x.shape[0], -1)\n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # Turn labels into their one-hot representations\n",
    "    y = one_hot_encoder(train_y)\n",
    "\n",
    "    # Make a data loader\n",
    "    trainloader = dataloader(x, y, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize weights\n",
    "    w1, b1 = initialize_weight((784, 256), bias=True)\n",
    "    w2, b2 = initialize_weight((256, 10), bias=True)\n",
    "\n",
    "    loss_history = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(\"Epoch {}/{}\\n===============\".format(epoch, num_epochs))\n",
    "\n",
    "        batch_loss = 0\n",
    "        acc = 0\n",
    "        for inputs, labels in trainloader:\n",
    "            # Number of samples per batch\n",
    "            m = inputs.shape[0]\n",
    "            \n",
    "            # Forward Prop\n",
    "            h1 = np.dot(inputs, w1) + b1\n",
    "            a1 = sigmoid(h1)\n",
    "            h2 = np.dot(a1, w2) + b2\n",
    "            a2 = softmax(h2)\n",
    "            out = a2\n",
    "\n",
    "            # Cross Entropy Loss\n",
    "            batch_loss += cross_entropy_loss(out, labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Compute Accuracy\n",
    "            pred = np.argmax(out, axis=1)\n",
    "            pred = pred.reshape(pred.shape[0], 1)\n",
    "            acc += np.sum(pred == labels.argmax(axis=1).reshape(m,1))\n",
    "\n",
    "            # Backward Prop\n",
    "            dh2 = a2 - labels \n",
    "            dw2 = (1/m) * np.dot(a1.T, dh2)\n",
    "            db2 = (1/m) * np.sum(dh2, axis=0, keepdims=True)\n",
    "\n",
    "            dh1 = np.dot(dh2, w2.T) * sigmoid_prime(a1)\n",
    "            dw1 = (1/m) * np.dot(inputs.T, dh1)\n",
    "            db1 = (1/m) * np.sum(dh1, axis=0, keepdims=True)\n",
    "\n",
    "            # Weight (and bias) update\n",
    "            w1 -= learning_rate * dw1\n",
    "            b1 -= learning_rate * db1\n",
    "            w2 -= learning_rate * dw2\n",
    "            b2 -= learning_rate * db2\n",
    "            \n",
    "        loss_history.append(batch_loss/num_samples)\n",
    "        print(\"Loss: {:.6f}\".format(batch_loss/num_samples))\n",
    "        print(\"Accuracy: {:.2f}%\\n\".format(acc/num_samples*100))\n",
    "\n",
    "    return w1, b1, w2, b2, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "===============\n",
      "Loss: 0.544662\n",
      "Accuracy: 85.32%\n",
      "\n",
      "Epoch 2/20\n",
      "===============\n",
      "Loss: 0.419286\n",
      "Accuracy: 88.20%\n",
      "\n",
      "Epoch 3/20\n",
      "===============\n",
      "Loss: 0.391707\n",
      "Accuracy: 88.89%\n",
      "\n",
      "Epoch 4/20\n",
      "===============\n",
      "Loss: 0.372604\n",
      "Accuracy: 89.13%\n",
      "\n",
      "Epoch 5/20\n",
      "===============\n",
      "Loss: 0.378077\n",
      "Accuracy: 88.97%\n",
      "\n",
      "Epoch 6/20\n",
      "===============\n",
      "Loss: 0.374995\n",
      "Accuracy: 88.88%\n",
      "\n",
      "Epoch 7/20\n",
      "===============\n",
      "Loss: 0.354260\n",
      "Accuracy: 89.49%\n",
      "\n",
      "Epoch 8/20\n",
      "===============\n",
      "Loss: 0.347341\n",
      "Accuracy: 89.76%\n",
      "\n",
      "Epoch 9/20\n",
      "===============\n",
      "Loss: 0.358557\n",
      "Accuracy: 89.32%\n",
      "\n",
      "Epoch 10/20\n",
      "===============\n",
      "Loss: 0.357023\n",
      "Accuracy: 89.28%\n",
      "\n",
      "Epoch 11/20\n",
      "===============\n",
      "Loss: 0.379309\n",
      "Accuracy: 88.51%\n",
      "\n",
      "Epoch 12/20\n",
      "===============\n",
      "Loss: 0.353578\n",
      "Accuracy: 89.27%\n",
      "\n",
      "Epoch 13/20\n",
      "===============\n",
      "Loss: 0.334911\n",
      "Accuracy: 89.98%\n",
      "\n",
      "Epoch 14/20\n",
      "===============\n",
      "Loss: 0.328657\n",
      "Accuracy: 90.15%\n",
      "\n",
      "Epoch 15/20\n",
      "===============\n",
      "Loss: 0.339763\n",
      "Accuracy: 89.71%\n",
      "\n",
      "Epoch 16/20\n",
      "===============\n",
      "Loss: 0.323087\n",
      "Accuracy: 90.37%\n",
      "\n",
      "Epoch 17/20\n",
      "===============\n",
      "Loss: 0.325826\n",
      "Accuracy: 90.22%\n",
      "\n",
      "Epoch 18/20\n",
      "===============\n",
      "Loss: 0.316581\n",
      "Accuracy: 90.47%\n",
      "\n",
      "Epoch 19/20\n",
      "===============\n",
      "Loss: 0.314759\n",
      "Accuracy: 90.33%\n",
      "\n",
      "Epoch 20/20\n",
      "===============\n",
      "Loss: 0.318970\n",
      "Accuracy: 90.34%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1_mbgd, b1_mbgd, w2_mbgd, b2_mbgd, loss_history_mbgd = train(train_x, train_y, learning_rate=0.1, num_epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1822a504dd8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot and Compare Losses\n",
    "plt.plot(loss_history_m, label=\"Moment\")\n",
    "plt.plot(loss_history_r, label=\"RMSProp\")\n",
    "plt.plot(loss_history_a, label=\"Adam\")\n",
    "plt.plot(loss_history_mbgd, label=\"mini-Batch Gradient Descent\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
